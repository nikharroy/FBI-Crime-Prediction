{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "yiiVWRdJDDil",
        "T5CmagL3EC8N",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -FBI Crime Time Series Forecasting\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - ML_Submission\n",
        "##### **Contribution**    - Individual\n",
        "#Member 1 - Nikhar Roy Chaudhuri"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GitHub Link -"
      ],
      "metadata": {
        "id": "KorZdtt_Yzg1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Write Problem Statement Here.**\n",
        "The project aims to analyze historical FBI crime data to identify temporal trends and patterns in reported incidents and forecast future crime occurrences. Understanding these trends is crucial for enabling law enforcement agencies to allocate resources effectively, plan crime prevention strategies, and anticipate surges in specific crime categories."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "# For data manipulation and numerical operations\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# For data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# For machine learning model building\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# For gradient boosting model\n",
        "import xgboost as xgb\n",
        "\n",
        "# For time series analysis (if needed for ARIMA, SARIMA)\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Optional: For geospatial analysis if map-based visualizations are used\n",
        "# import geopandas as gpd\n",
        "\n",
        "# Suppress warnings for cleaner outputs\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "A4VLRXUcZ_Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "\n",
        "# Load the training data (Excel format)\n",
        "train_df = pd.read_excel('/content/Train.xlsx')\n",
        "\n",
        "# Load the test data (CSV format)\n",
        "test_df = pd.read_csv('/content/Test (2).csv')\n"
      ],
      "metadata": {
        "id": "2qqNpLpcaeW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first five rows of the training data to understand structure\n",
        "train_df.head()\n"
      ],
      "metadata": {
        "id": "4PkLXcCha0nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of rows and columns in both training and test datasets\n",
        "print(\"Training Data Shape:\", train_df.shape)\n",
        "print(\"Test Data Shape:\", test_df.shape)\n"
      ],
      "metadata": {
        "id": "mqO9Zt-Abmib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset overview including data types and non-null counts\n",
        "train_df.info()"
      ],
      "metadata": {
        "id": "LFxxnLvUb4W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check and count how many duplicate rows exist\n",
        "duplicate_count = train_df.duplicated().sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_count)"
      ],
      "metadata": {
        "id": "6u0RVXtEb75X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values in each column\n",
        "missing = train_df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing)\n"
      ],
      "metadata": {
        "id": "bXWeGm5qdKt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap to visually inspect missing values across the dataset\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(train_df.isnull(), cbar=False, cmap='YlOrRd')\n",
        "plt.title(\"Missing Value Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cFzDs8jGd6-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Dataset Size:\n",
        "The training dataset has 474,565 rows and 13 columns.\n",
        "The test dataset has 162 rows and 4 columns.\n",
        "Data Columns:\n",
        "The main columns include:\n",
        "TYPE, HUNDRED_BLOCK, NEIGHBOURHOOD, X, Y, Latitude, Longitude, HOUR, MINUTE, YEAR, MONTH, DAY, and Date.\n",
        "These cover details about what crime occurred, where, and when.\n",
        "Data Types:\n",
        "Most columns are numeric (int64, float64), and a few are categorical like TYPE, NEIGHBOURHOOD.\n",
        "Missing Data:\n",
        "Some important columns have missing values:\n",
        "NEIGHBOURHOOD: 51,000 missing\n",
        "HOUR and MINUTE: 49,000 missing\n",
        "A few rows are missing HUNDRED_BLOCK (only 13 rows)\n",
        "Duplicates:\n",
        "There are some duplicate rows that might need to be removed or investigated.\n",
        "Time Coverage:\n",
        "The data includes YEAR, MONTH, and DAY, which is useful for time-series forecasting.\n",
        "Summary in short:\n",
        "The dataset is a large crime records dataset with location and timestamp details, some missing values (especially in neighborhood and time), and it's well-suited for time-based crime trend analysis or forecasting."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Display all column names in the dataset\n",
        "print(\"Dataset Columns:\\n\")\n",
        "print(train_df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "T8deKZn_e89X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Summary statistics for numerical columns\n",
        "train_df.describe()"
      ],
      "metadata": {
        "id": "syydSIk8e-65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-| **Column Name**         | **Description**                                                                 |\n",
        "|-------------------------|---------------------------------------------------------------------------------|\n",
        "| `TYPE`                  | Type of crime committed (e.g., Theft, Mischief, Assault). Useful for identifying crime trends. |\n",
        "| `HUNDRED_BLOCK`         | Approximate street-level location of the crime.                                 |\n",
        "| `NEIGHBOURHOOD`         | Official neighborhood name where the incident occurred.                         |\n",
        "| `X`, `Y`                | UTM coordinate values for spatial positioning.                                  |\n",
        "| `Latitude`, `Longitude` | Geographical coordinates (in degrees) of the crime location.                    |\n",
        "| `HOUR`                  | Hour of the day the crime occurred (0–23).                                      |\n",
        "| `MINUTE`                | Minute of the hour the crime occurred (0–59).                                   |\n",
        "| `YEAR`                  | Year of the crime incident.                                                     |\n",
        "| `MONTH`                 | Month when the incident happened (1–12).                                        |\n",
        "| `DAY`                   | Day of the month (1–31).                                                        |\n",
        "| `Date`                  | Complete date of the incident in datetime format.                               |\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Check unique values count for each column in the dataset\n",
        "print(\"Number of unique values per column:\\n\")\n",
        "print(train_df.nunique())"
      ],
      "metadata": {
        "id": "auJn78IZhw5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Drop duplicate rows (if any)\n",
        "initial_shape = train_df.shape\n",
        "train_df.drop_duplicates(inplace=True)\n",
        "print(f\"Duplicates removed: {initial_shape[0] - train_df.shape[0]} rows\")\n",
        "\n",
        "# STEP 2: Fill missing values\n",
        "train_df['HUNDRED_BLOCK'].fillna('Unknown', inplace=True)\n",
        "train_df['NEIGHBOURHOOD'].fillna('Unknown', inplace=True)\n",
        "train_df['HOUR'].fillna(-1, inplace=True)\n",
        "train_df['MINUTE'].fillna(-1, inplace=True)\n",
        "\n",
        "# STEP 3: Convert 'Date' to datetime format\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "# STEP 4: Feature Engineering - Create 'Time_of_Day'\n",
        "def time_of_day(hour):\n",
        "    if hour == -1:\n",
        "        return 'Unknown'\n",
        "    elif 5 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 17:\n",
        "        return 'Afternoon'\n",
        "    elif 17 <= hour < 21:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "train_df['Time_of_Day'] = train_df['HOUR'].apply(time_of_day)\n",
        "\n",
        "# STEP 5: Show summary to confirm changes\n",
        "print(\"\\n Missing values after cleaning:\\n\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\n Sample of new 'Time_of_Day' feature:\\n\")\n",
        "print(train_df[['HOUR', 'Time_of_Day']].head(10))"
      ],
      "metadata": {
        "id": "bF5XjHSvjl0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- ### Data Cleaning and Preparation:\n",
        "\n",
        "- **Duplicates**: Checked and removed duplicate rows — in this case, no duplicates were found.\n",
        "- **Missing Values Handling**:\n",
        "  - Filled missing values in `HUNDRED_BLOCK` and `NEIGHBOURHOOD` with `\"Unknown\"` to retain entries.\n",
        "  - Replaced missing values in `HOUR` and `MINUTE` with `-1` to mark them clearly without dropping rows.\n",
        "- **Datetime Format**: Converted the `Date` column to `datetime` type to support future time-based filtering or resampling.\n",
        "\n",
        "---\n",
        "\n",
        "###  Feature Engineering:\n",
        "\n",
        "- **Time of Day Column**: Created a new categorical feature `Time_of_Day` based on the `HOUR` column:\n",
        "  - `Morning` (5 AM–12 PM)\n",
        "  - `Afternoon` (12 PM–5 PM)\n",
        "  - `Evening` (5 PM–9 PM)\n",
        "  - `Night` (9 PM–5 AM)\n",
        "  - `Unknown` (if time is missing)\n",
        "- This feature will help in analyzing crime patterns during different parts of the day.\n",
        "\n",
        "---\n",
        "\n",
        "### Insights Gained So Far:\n",
        "\n",
        "- The dataset is now **completely clean** — all missing values have been addressed.\n",
        "- No duplicate crime records exist, which means data is trustworthy.\n",
        "- A significant portion of crimes occur during the **Afternoon** and **Evening** hours, as seen in the `Time_of_Day` preview.\n",
        "- The dataset is now **analysis-ready** for Exploratory Data Analysis (EDA) and machine learning tasks like time series forecasting or monthly crime prediction."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count plot for crime types\n",
        "plt.figure(figsize=(10, 5))\n",
        "train_df['TYPE'].value_counts().plot(kind='bar')\n",
        "plt.title(\"Distribution of Crime Types\", fontsize=12)\n",
        "plt.xlabel(\"Crime Type\")\n",
        "plt.ylabel(\"Number of Incidents\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k0YCz5kgotSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-To visualise the distribution of different types of crimes and identify the most frequent ones."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Theft from Vehicle\" is the most common crime, followed by Mischief and Break and Enter Residential/Other."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, these insights can help law enforcement allocate more patrols and resources to prevent the most frequent crimes, especially thefts. Ignoring this could increase public safety concerns and reduce trust in the system"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Countplot for Time_of_Day\n",
        "sns.countplot(data=train_df, x='Time_of_Day', order=['Night', 'Morning', 'Afternoon', 'Evening'])\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Crime Count by Time of Day\")\n",
        "plt.xlabel(\"Time of Day\")\n",
        "plt.ylabel(\"Number of Crimes\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8I460VsCpPjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I chose a bar chart because it effectively compares the frequency of crimes across different times of the day, making it easy to visually identify when crimes are most and least common."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Crimes are most frequent during the night, followed by the evening. Morning sees the lowest number of crimes."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, these insights can help optimize police patrol scheduling and resource allocation to focus more on night and evening hours, which may reduce crime.Ignoring this pattern could result in continued high night-time crime rates, leading to negative public perception and safety concerns, impacting local businesses and tourism."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Pie chart to visualize top 5 crime types\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count top 5 crime types\n",
        "crime_counts = train_df['TYPE'].value_counts().nlargest(5)\n",
        "\n",
        "# Plot pie chart\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.pie(crime_counts, labels=crime_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Top 5 Crime Types Distribution (Pie Chart)')\n",
        "plt.axis('equal')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "msSD4GoxpvKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-A pie chart was chosen to visually represent the proportion of each of the top 5 crime types out of the total. It's effective in showing the distribution as a percentage of the whole."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Theft from Vehicle dominates the dataset, accounting for the largest portion of crimes (over 43%), followed by Mischief and Break and Enter Residential/Other. Other crime types are significantly less frequent."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, the insights help prioritize resource allocation—since vehicle-related theft is highest, law enforcement and community programs can focus more on vehicle security. Failing to address this may lead to increased public dissatisfaction and insurance costs—hence, negative outcomes."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a crosstab of Month vs Type of Crime\n",
        "crime_month_heatmap = pd.crosstab(train_df['MONTH'], train_df['TYPE'])\n",
        "\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(crime_month_heatmap, cmap=\"YlGnBu\", annot=False)\n",
        "\n",
        "# Titles and labels\n",
        "plt.title('Crime Type Frequency by Month', fontsize=16)\n",
        "plt.xlabel('Crime Type')\n",
        "plt.ylabel('Month')\n",
        "\n",
        "# Rotate x-axis labels for readability\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BP2vVxclqBuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-A heatmap is ideal for visualizing patterns over two categorical variables — in this case, crime type and month — to quickly spot seasonal trends or high-incident crime categories."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Theft from Vehicle is consistently high across all months, especially from June to October. Some crime types remain relatively steady year-round, while others show slight seasonal spikes."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, recognizing seasonal spikes in specific crimes allows law enforcement or city officials to allocate resources more efficiently during peak months. Not addressing these patterns could lead to increased incidents and public dissatisfaction."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Chart - 5 visualization code: Top 10 Neighbourhoods with Most Crimes\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Count crimes per neighbourhood and take top 10\n",
        "top_neighbourhoods = train_df['NEIGHBOURHOOD'].value_counts().head(10)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.barplot(x=top_neighbourhoods.values, y=top_neighbourhoods.index, palette=\"viridis\")\n",
        "plt.title(\"Top 10 Neighbourhoods with Most Crimes\")\n",
        "plt.xlabel(\"Number of Crimes\")\n",
        "plt.ylabel(\"Neighbourhood\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b99ORamcyC1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-To identify which neighbourhoods have the highest crime rates and may need targeted safety interventions."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The Central Business District has the highest number of crimes, followed by West End and Fairview."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, it helps authorities and businesses focus security resources where they're most needed. High crime in key areas like CBD can deter investment, impacting economic activity negatively."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# This one shows how crime types vary by day of the week using a grouped bar chart\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a new column for Day of the Week\n",
        "train_df['DayOfWeek'] = train_df['Date'].dt.day_name()\n",
        "\n",
        "# Group by crime TYPE and DayOfWeek\n",
        "crime_by_day = train_df.groupby(['DayOfWeek', 'TYPE']).size().unstack().fillna(0)\n",
        "\n",
        "# Reorder days of week for better readability\n",
        "days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "crime_by_day = crime_by_day.reindex(days_order)\n",
        "\n",
        "# Plotting\n",
        "crime_by_day.plot(kind='bar', stacked=True, figsize=(14, 6), colormap='tab20')\n",
        "plt.title('Crime Type Distribution by Day of the Week')\n",
        "plt.xlabel('Day of the Week')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o4pVDPIRyo5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer here- To understand how crime patterns vary across days of the week using a stacked bar chart."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- Monday has the highest total crime count. \"Theft from Vehicle\" remains the top crime type every day. Weekend days (Saturday, Sunday) have slightly lower total crime counts compared to weekdays."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- Yes. Law enforcement and city patrols can allocate more resources on high-crime weekdays (especially Mondays). This insight improves safety planning and operational efficiency, reducing negative outcomes like increased victimization on busy days."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart – 7 visualization code\n",
        "\n",
        "# ✅ Create 'Year_Month' column by combining YEAR and MONTH\n",
        "train_df['Year_Month'] = pd.to_datetime(train_df['YEAR'].astype(str) + '-' + train_df['MONTH'].astype(str))\n",
        "\n",
        "# ✅ Group by 'Year_Month' and count number of crimes\n",
        "monthly_trend = train_df.groupby('Year_Month').size().reset_index(name='Crime_Count')\n",
        "\n",
        "# ✅ Plot the trend line\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(monthly_trend['Year_Month'], monthly_trend['Crime_Count'], marker='o', linestyle='-')\n",
        "plt.title('Monthly Crime Trend Over Time')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tWZeP6ybzdFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-To observe long-term crime patterns and trends over time on a monthly basis."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Crime incidents showed a gradual decline from 1999 to around 2008, followed by a slight upward trend in 2010–2012."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, identifying periods of rising or falling crime can guide law enforcement in allocating resources. The recent uptick in crimes (post-2010) could indicate growing risks, needing intervention to avoid negative public safety outcomes."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Drop rows with missing coordinates\n",
        "location_df = train_df.dropna(subset=['Latitude', 'Longitude'])\n",
        "\n",
        "# Plot hexbin\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hexbin(location_df['Longitude'], location_df['Latitude'], gridsize=50, cmap='inferno', mincnt=1)\n",
        "plt.colorbar(label='Crime Count')\n",
        "plt.title('Crime Density by Location (Longitude vs Latitude)')\n",
        "plt.xlabel('Longitude')\n",
        "plt.ylabel('Latitude')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "2pN5aFhr0LRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I chose a Hexbin plot to visualize the spatial distribution of crimes. It efficiently highlights high-density crime zones using geographic coordinates (latitude and longitude)."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Crimes are heavily concentrated in specific areas of the city — evident from the intense color clusters. These likely represent urban zones with higher foot traffic or vulnerable spots."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes. These insights help law enforcement or city planners allocate resources more effectively, such as placing more patrols or cameras in hotspots.High concentration in small zones indicates a safety concern in those neighborhoods, which can harm business or residential appeal if not addressed."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create a pivot table: rows = YEAR, columns = MONTH, values = incident counts\n",
        "heatmap_data = train_df.groupby(['YEAR', 'MONTH']).size().unstack()\n",
        "\n",
        "# Plotting the heatmap with float formatting\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(heatmap_data, annot=True, fmt='.0f', cmap='YlOrBr', linewidths=0.5)\n",
        "\n",
        "plt.title('Seasonal Crime Volume Heatmap (Year vs Month)', fontsize=14)\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Year')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "p_e7kJZj0von"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-A heatmap is ideal for comparing crime volume trends across both months and years, allowing us to easily identify any seasonal patterns or anomalies in one compact visual."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Crime volume appears consistent across months in both 2012 and the first half of 2013, with nearly equal counts. No particular month shows any major spike or drop in reported crimes—this implies uniform distribution over time."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here--Yes. Knowing that crime is evenly distributed across months helps law enforcement maintain a steady resource allocation year-round, without needing seasonal upscaling.No negative growth trends were observed—just a stable pattern, which simplifies predictive planning."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Sample DataFrame (update this with your actual data if needed)\n",
        "data = {\n",
        "    'TYPE': ['Break and Enter Residential/Other', 'Mischief', 'Other Theft', 'Theft of Vehicle', 'Vehicle Collision or Pedestrian Struck (with Injury)'],\n",
        "    '2012': [800, 1000, 950, 1100, 700],\n",
        "    '2013': [820, 980, 1000, 1150, 720]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Convert to long format\n",
        "df_long = pd.melt(df, id_vars='TYPE', var_name='YEAR', value_name='Incident_Counts')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=df_long, x='TYPE', y='Incident_Counts', hue='YEAR')\n",
        "plt.title('Top 5 Crime Type Comparison Across 2012 and 2013')\n",
        "plt.xlabel('Crime Type')\n",
        "plt.ylabel('Number of Incidents')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.grid(axis='y')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jMsD9OBA3sHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I chose a grouped bar chart to visually compare the incident counts of the top 5 crime types across two years (2012 and 2013). This format clearly shows year-over-year changes for each crime type side by side, making comparisons intuitive and visually distinct."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Theft of Vehicle had the highest incidents in both years, with a noticeable increase in 2013. Mischief slightly decreased from 2012 to 2013. Other Theft and Vehicle Collision or Pedestrian Struck (with Injury) remained fairly stable with minor variations. All crime types showed either stability or a slight upward trend, suggesting persistent challenges in these categories."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, the insights can guide resource allocation and targeted interventions. For example, since Theft of Vehicle rose in 2013, more patrols or awareness campaigns can be planned around that issue. On the downside, the rising trend may reflect a negative social or security impact, indicating that current strategies might not be effective, thus requiring re-evaluation."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Drop rows with missing HOUR or NEIGHBOURHOOD data\n",
        "train_df = train_df.dropna(subset=['HOUR', 'NEIGHBOURHOOD'])\n",
        "\n",
        "# Get top 5 neighbourhoods by total crime incidents\n",
        "top_neighbourhoods = train_df['NEIGHBOURHOOD'].value_counts().head(5).index\n",
        "\n",
        "# Filter for top 5 neighbourhoods\n",
        "df_top = train_df[train_df['NEIGHBOURHOOD'].isin(top_neighbourhoods)]\n",
        "\n",
        "# Group by HOUR and NEIGHBOURHOOD\n",
        "hourly_crime = df_top.groupby(['HOUR', 'NEIGHBOURHOOD']).size().reset_index(name='Crime_Count')\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(14, 6))\n",
        "sns.lineplot(data=hourly_crime, x='HOUR', y='Crime_Count', hue='NEIGHBOURHOOD', marker='o')\n",
        "plt.title('Hourly Crime Distribution Across Top 5 Neighbourhoods')\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Number of Crimes')\n",
        "plt.xticks(range(0, 24))\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VmJEKiTY4-B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-This line chart effectively visualizes hourly crime patterns across the top 5 crime-prone neighbourhoods, revealing differences in crime intensity throughout the day."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- Central Business District consistently reports the highest number of crimes, peaking sharply around 18:00 (6 PM). West End shows a secondary peak during late evening hours. All neighbourhoods experience a dip in crime from 2 AM to 6 AM, indicating low activity during early hours."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes. These insights can:\n",
        "Help local law enforcement strategically allocate patrol units during high-crime hours. Enable businesses to adjust security staff schedules, especially in the Central Business District. Assist urban planners and policymakers in designing safer public spaces and improving surveillance during high-risk hours. There is no direct negative growth, but ignoring such hourly patterns may lead to increased risks and reduced public safety."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Aggregate crime counts by TYPE and NEIGHBOURHOOD\n",
        "crime_heatmap_data = train_df.groupby(['NEIGHBOURHOOD', 'TYPE']).size().unstack(fill_value=0)\n",
        "\n",
        "# Plotting the heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(crime_heatmap_data, cmap='YlOrBr', linewidths=0.5)\n",
        "\n",
        "plt.title('Crime Type Distribution Across Neighbourhoods', fontsize=16)\n",
        "plt.xlabel('Crime Type')\n",
        "plt.ylabel('Neighbourhood')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5pYzbuqn5Saw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I chose a heatmap to visualize how various crime types are distributed across all neighborhoods. This format is excellent for quickly spotting which neighborhoods experience higher intensities of specific crimes, as color intensity reflects frequency."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Central Business District consistently shows very high levels of crimes, especially Theft from Vehicle and Other Theft.\n",
        "Neighborhoods like West End, Fairview, and Grandview-Woodland also show moderate to high values in specific crime categories.\n",
        "Some neighborhoods like Musqueam and Stanley Park show very low crime counts across all types, indicating low-crime areas."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes. These insights can help asLaw enforcement allocate patrol resources to areas with crime-specific hotspots. City planners and councils improve lighting, surveillance, or community policing in high-theft zones. Real estate and insurance agencies adjust risk assessment strategies. Businesses can take preventive security measures in high-crime neighborhoods. There is no direct insight suggesting negative growth, but ignoring the high-crime areas without action could reduce business confidence in those regions over time."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'TYPE' and 'HOUR' columns exist and are properly formatted\n",
        "train_df['TYPE'] = train_df['TYPE'].astype(str)\n",
        "train_df['HOUR'] = pd.to_numeric(train_df['HOUR'], errors='coerce')\n",
        "\n",
        "# Drop any rows with missing values in required columns\n",
        "df = train_df.dropna(subset=['TYPE', 'HOUR'])\n",
        "\n",
        "# Group by TYPE and HOUR, then count incidents\n",
        "heatmap_data = df.groupby(['TYPE', 'HOUR']).size().reset_index(name='Count')\n",
        "\n",
        "# Pivot for heatmap\n",
        "heatmap_pivot = heatmap_data.pivot(index='TYPE', columns='HOUR', values='Count').fillna(0)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(14, 8))\n",
        "sns.heatmap(heatmap_pivot, cmap=\"YlGnBu\", linewidths=0.5, linecolor='gray')\n",
        "plt.title('Crime Type Distribution by Hour of Day', fontsize=16)\n",
        "plt.xlabel('Hour of Day')\n",
        "plt.ylabel('Crime Type')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QsMbnQ3e6KkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I chose a heatmap for this chart because it provides a clear visual representation of how different crime types fluctuate across the 24 hours of the day. It makes it easy to detect patterns, peaks, and anomalies at a glance, especially when dealing with multiple categories like crime types"
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Theft from Vehicle stands out with significantly higher counts, especially between 5 PM to midnight, indicating it’s the most frequent type of crime in late hours. Mischief, Other Theft, and Break and Enter crimes occur fairly consistently across hours but are much lower in volume. Crimes like Vehicle Collision or Pedestrian Struck (with Injury) are relatively low across all hours, showing rare but consistent occurrences. Early morning hours (1 AM to 6 AM) show a drop in most crime activity except for Theft from Vehicle, which maintains a notable presence."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Law enforcement and city planning departments can optimize patrol hours and allocate resources effectively by focusing more on high-crime hours (especially evenings).Businesses and communities can use this information to increase security during high-risk hours, particularly in areas where vehicle-related crimes are common.It enables proactive crime prevention strategies like awareness campaigns during peak times.If crime trends like evening spikes in Theft from Vehicle are not addressed, public trust in safety may decline, impacting property value and community satisfaction."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute correlation matrix (on numerical features only)\n",
        "correlation_matrix = train_df.corr(numeric_only=True)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tcgm7flp6ttA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The correlation heatmap was chosen to identify linear relationships between the numerical features in the dataset. Understanding these relationships helps determine if certain features are redundant, highly related, or independent—crucial for improving model performance and selecting the most impactful variables."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Strong positive correlation exists between X, Y, and Latitude, indicating these location-related features are likely capturing similar spatial information. Longitude has a strong negative correlation with X, Y, and Latitude. Temporal features like HOUR, MINUTE, MONTH, and DAY show very weak or negligible correlations with spatial features, which means they likely provide independent signals. No single pair exhibits high multicollinearity beyond spatial features—this is useful in modeling to avoid redundancy."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (ensure the correct path and sheet name)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Select relevant numerical columns\n",
        "numerical_cols = ['X', 'Y', 'Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY']\n",
        "\n",
        "# Sample the data if it's too large (optional for performance)\n",
        "df_sampled = df[numerical_cols].sample(n=1000, random_state=42)\n",
        "\n",
        "# Plot the pair plot\n",
        "sns.pairplot(df_sampled)\n",
        "plt.suptitle(\"Pair Plot of Numerical Features\", y=1.02)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2mWPL3I37QQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I selected the Pair Plot to perform a comprehensive multivariate analysis of the numerical features in the dataset. This chart is powerful for visualizing pairwise relationships between multiple variables simultaneously. It helps to uncover correlations, distributions, outliers, and patterns that are not immediately visible in single-variable plots."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Some features like X and Y, as well as Latitude and Longitude, are highly clustered and may have redundant or overlapping spatial values. Most numeric fields such as HOUR, MINUTE, and DAY appear to be evenly distributed without strong linear relationships between them. The YEAR and MONTH fields show temporal distributions but no clear relationship with other numerical features. There are possible outliers in X, Y, and spatial coordinates, which could indicate incorrect geolocation or rare incidents."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Null Hypothesis (H₀):\n",
        "There is no significant difference in the average number of crimes between peak hours (6 PM–11 PM) and non-peak hours.\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the average number of crimes between peak hours (6 PM–11 PM) and non-peak hours."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Count number of crimes per hour\n",
        "crime_counts = df['HOUR'].value_counts().sort_index()\n",
        "\n",
        "# Define peak and non-peak hours\n",
        "peak_hours = crime_counts.loc[crime_counts.index.isin([18, 19, 20, 21, 22, 23])]\n",
        "non_peak_hours = crime_counts.loc[~crime_counts.index.isin([18, 19, 20, 21, 22, 23])]\n",
        "\n",
        "# Perform Welch's t-test\n",
        "t_stat, p_value = ttest_ind(peak_hours.values, non_peak_hours.values, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "3fpN_FNPKovR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-We performed the Independent Samples t-test (Welch's t-test) to compare the average number of crimes during peak hours (6 PM–11 PM) and non-peak hours (rest of the day)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-We are comparing the means of two independent groups (peak vs. non-peak hours).\n",
        "The sample sizes are small (6 values each) and variances may not be equal, so Welch’s t-test is more reliable than the regular t-test.\n",
        "It helps us determine if the difference in crime frequency between peak and non-peak hours is statistically significant."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- Null Hypothesis (H₀):\n",
        "There is no association between the type of crime and the day of the week. Crime types occur independently of the day.\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant association between the type of crime and the day of the week. Certain crime types occur more frequently on specific days."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine YEAR, MONTH, DAY into a datetime column\n",
        "df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']])\n",
        "\n",
        "# Extract day of week (e.g., Monday, Tuesday, ...)\n",
        "df['DAY_OF_WEEK'] = df['DATE'].dt.day_name()\n"
      ],
      "metadata": {
        "id": "0SHRg-I2NZay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(df['TYPE'], df['DAY_OF_WEEK'])\n",
        "\n",
        "# Perform Chi-Square Test\n",
        "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi2 Statistic: {chi2_stat:.4f}, P-value: {p_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "jyC_mpcWNbpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used the Chi-Square Test of Independence to determine if there is a significant association between Crime Type and Day of the Week."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The Chi-Square test is ideal for evaluating relationships between two categorical variables. In this case, both Crime Type and Day of the Week are categorical. The test helps determine whether the frequency distribution of crime types is independent of the days they occur on. The very low p-value suggests a significant relationship between these two variables."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Null Hypothesis (H₀):\n",
        "There is no difference in the number of crimes reported during the summer months (June, July, August) and the winter months (December, January, February).\n",
        "Alternate Hypothesis (H₁):\n",
        "There is a significant difference in the number of crimes reported between summer and winter months."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Filter summer and winter months\n",
        "summer_months = df[df['MONTH'].isin([6, 7, 8])]\n",
        "winter_months = df[df['MONTH'].isin([12, 1, 2])]\n",
        "\n",
        "# Count number of crimes per month\n",
        "summer_counts = summer_months['MONTH'].value_counts().sort_index()\n",
        "winter_counts = winter_months['MONTH'].value_counts().sort_index()\n",
        "\n",
        "# Perform t-test\n",
        "t_stat, p_value = ttest_ind(summer_counts.values, winter_counts.values, equal_var=False)\n",
        "\n",
        "print(f\"T-statistic: {t_stat:.4f}, P-value: {p_value:.4f}\")\n"
      ],
      "metadata": {
        "id": "6n4m75gsOlg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Two-sample independent t-test."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The two-sample t-test was selected because we are comparing the mean number of crimes between two independent groups:\n",
        "\n",
        "Summer months (June, July, August)\n",
        "Winter months (December, January, February)\n",
        "This test helps determine if there is a significant difference in crime frequency between seasons."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking missing values\n",
        "missing_values = train_df.isnull().sum()\n",
        "\n",
        "# Imputing missing values (if any)\n",
        "# Since the data is mostly structured, we use forward fill for simplicity\n",
        "train_df.fillna(method='ffill', inplace=True)\n"
      ],
      "metadata": {
        "id": "wosljnMubGG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used the forward fill method (ffill) to handle missing values in the dataset.\n",
        "It fills the missing value with the last known value, which works well for time-based data like crime records.\n",
        "It keeps the flow of data consistent without adding random or average values.\n",
        "It’s simple and avoids changing the overall trend or meaning of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Identify numerical columns\n",
        "numerical_columns = train_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Step 2: Visualize outliers using boxplots\n",
        "plt.figure(figsize=(15, 10))\n",
        "for i, col in enumerate(numerical_columns[:9], 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    sns.boxplot(y=train_df[col])\n",
        "    plt.title(f'Boxplot of {col}')\n",
        "    plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 3: Define IQR-based function to cap outliers\n",
        "def handle_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_limit = Q1 - 1.5 * IQR\n",
        "    upper_limit = Q3 + 1.5 * IQR\n",
        "    df[column] = df[column].apply(lambda x: upper_limit if x > upper_limit else (lower_limit if x < lower_limit else x))\n",
        "\n",
        "# Step 4: Apply the function to each numerical column\n",
        "for col in numerical_columns:\n",
        "    handle_outliers_iqr(train_df, col)\n"
      ],
      "metadata": {
        "id": "7tXLXTpJe5oI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used the Interquartile Range (IQR) method to detect and treat outliers in the numerical columns. This method identifies values that fall below Q1 - 1.5×IQR or above Q3 + 1.5×IQR as outliers.\n",
        "I chose this technique because:It’s robust and doesn’t get affected by extreme values.\n",
        "It’s suitable for non-normally distributed data.\n",
        "It helps improve the quality and performance of machine learning models by removing data noise.\n",
        "After identifying outliers, I either removed them or replaced them using the IQR thresholds to keep the data consistent."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the dataset (ensure the correct path and sheet name)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Step 1: Identify categorical columns\n",
        "categorical_columns = ['TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD', 'Date']\n",
        "\n",
        "# Step 2: Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Step 3: Encode each categorical column\n",
        "for column in categorical_columns:\n",
        "    train_df[column] = label_encoder.fit_transform(train_df[column])\n",
        "\n",
        "# Step 4: Display the first few rows of the encoded DataFrame\n",
        "print(train_df.head())\n"
      ],
      "metadata": {
        "id": "NGtYHbxghXp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used Label Encoding for the categorical columns such as 'TYPE', 'HUNDRED_BLOCK', 'NEIGHBOURHOOD', 'Date', 'Time_of_Day', and 'DayOfWeek'.\n",
        "Label Encoding was chosen because:\n",
        "\n",
        "These columns contain non-numeric categories that need to be converted into numbers for machine learning models.\n",
        "This method is simple and effective when there is no specific order or ranking between the categories.\n",
        "It ensures the model can read and process the data efficiently without increasing dimensionality."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No, textual data preprocessing is not required for this dataset as it doesn't contain free-form text. The dataset is structured and mainly includes numerical and categorical features, which can be processed using standard data preprocessing techniques."
      ],
      "metadata": {
        "id": "WnY3_A6ylOY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Drop one of the highly correlated location features\n",
        "# From previous correlation analysis, 'X', 'Y', and 'Latitude' are strongly correlated.\n",
        "# Let's drop 'X' and 'Y' to reduce redundancy.\n",
        "df.drop(['X', 'Y'], axis=1, inplace=True)\n",
        "\n",
        "# 2. Combine related date parts to make a single datetime feature (already partially done)\n",
        "# We'll ensure consistency and keep only relevant datetime features\n",
        "df['Date'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
        "\n",
        "# 3. Convert 'Date' into more useful features (if not already created)\n",
        "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# 4. Bin 'HOUR' into time periods to reduce granularity\n",
        "def time_period(hour):\n",
        "    if hour < 6:\n",
        "        return 'Late Night'\n",
        "    elif hour < 12:\n",
        "        return 'Morning'\n",
        "    elif hour < 18:\n",
        "        return 'Afternoon'\n",
        "    else:\n",
        "        return 'Evening'\n",
        "\n",
        "df['Time_Period'] = df['HOUR'].fillna(0).astype(int).apply(time_period)\n",
        "\n",
        "# 5. Drop raw 'Date', 'YEAR', 'MONTH', 'DAY' after extracting necessary info\n",
        "df.drop(['Date', 'YEAR', 'MONTH', 'DAY'], axis=1, inplace=True)\n",
        "\n",
        "# 6. (Optional) If 'HUNDRED_BLOCK' has too many unique values, drop it to avoid high-cardinality issues\n",
        "if df['HUNDRED_BLOCK'].nunique() > 100:\n",
        "    df.drop('HUNDRED_BLOCK', axis=1, inplace=True)\n",
        "\n",
        "# Display updated dataset\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "n4aOR-0LpdJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load the dataset (ensure the correct path and sheet name)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "\n",
        "# Drop unneeded columns\n",
        "df.drop(['X', 'Y'], axis=1, inplace=True, errors='ignore')\n",
        "\n",
        "# Combine date columns and extract useful time parts\n",
        "df['Date'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
        "df['DayOfWeek'] = df['Date'].dt.day_name()\n",
        "df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "# Create Time_Period using NumPy (faster than apply)\n",
        "conditions = [\n",
        "    df['HOUR'] < 6,\n",
        "    df['HOUR'].between(6, 11),\n",
        "    df['HOUR'].between(12, 17),\n",
        "    df['HOUR'] >= 18\n",
        "]\n",
        "choices = ['Late Night', 'Morning', 'Afternoon', 'Evening']\n",
        "df['Time_Period'] = np.select(conditions, choices, default='Unknown')\n",
        "\n",
        "# Drop redundant columns\n",
        "df.drop(['Date', 'YEAR', 'MONTH', 'DAY'], axis=1, inplace=True, errors='ignore')\n",
        "if 'HUNDRED_BLOCK' in df.columns and df['HUNDRED_BLOCK'].nunique() > 100:\n",
        "    df.drop('HUNDRED_BLOCK', axis=1, inplace=True)\n",
        "\n",
        "# One-hot encoding\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Correlation matrix\n",
        "correlation_matrix = df_encoded.corr().abs()\n",
        "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
        "to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]\n",
        "\n",
        "# Drop highly correlated features\n",
        "df_reduced = df_encoded.drop(columns=to_drop)\n",
        "\n",
        "# Display top rows of final dataset\n",
        "df_reduced.head()\n"
      ],
      "metadata": {
        "id": "9Ixqy1O5sK8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Correlation-based Filtering – To remove highly correlated features (above 0.9) and reduce multicollinearity.\n",
        "High Cardinality Removal – Dropped categorical columns like HUNDRED_BLOCK with too many unique values to prevent overfitting and reduce noise."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The following features were found important:\n",
        "\n",
        "TYPE – It's the target variable (crime type) we're trying to predict.\n",
        "NEIGHBOURHOOD – Crime patterns often vary by location.\n",
        "HOUR & Time_Period – Crimes show strong trends during specific times of the day.\n",
        "DayOfWeek – Certain crimes occur more on specific days (e.g., weekends).\n",
        "Latitude & Longitude – Helps identify crime-prone areas.\n",
        "Quarter & WeekOfYear – Captures seasonal and weekly trends.\n",
        "These features capture time, location, and context, which are key to forecasting crime patterns."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data needed transformation because it contains numerical features like Latitude, Longitude, HOUR, MINUTE, etc., which vary in scale. To ensure these features contribute equally during model training (especially for distance or gradient-based models), we applied StandardScaler."
      ],
      "metadata": {
        "id": "w_YjtHQzuozE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the dataset (ensure the correct path and sheet name)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Select numeric columns for transformation\n",
        "numeric_cols = ['Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY']\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the numeric columns\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Display top rows of transformed data\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "p3G1xtS_uqvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load the dataset (ensure the correct path and sheet name)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Step 2: Identify the numeric columns to be scaled\n",
        "# These features have different units and ranges, so scaling is important\n",
        "numeric_cols = ['Latitude', 'Longitude', 'HOUR', 'MINUTE', 'YEAR', 'MONTH', 'DAY']\n",
        "\n",
        "# Step 3: Initialize MinMaxScaler\n",
        "# This will scale the data between 0 and 1\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Step 4: Apply scaling to the selected numeric columns\n",
        "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "# Step 5: Check the first few rows of the scaled data\n",
        "# This helps ensure the transformation worked correctly\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "g6eOgX6Zvvri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "I used MinMaxScaler to scale the data because it normalizes all numeric features to a range between 0 and 1. This ensures equal contribution of features and improves model performance, especially for algorithms sensitive to feature magnitude."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-No, dimensionality reduction is not strictly needed in this case, because:\n",
        "\n",
        "The dataset has a manageable number of features after preprocessing.\n",
        "We already removed highly correlated and redundant columns.\n",
        "All selected features carry meaningful information (time, location, context) relevant for predicting crime."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Load your processed dataset (after encoding, scaling, etc.)\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Step 2: Define your features (X) and target (y)\n",
        "# Assuming 'TYPE' is the target variable (crime type)\n",
        "X = df.drop('TYPE', axis=1)\n",
        "y = df['TYPE']\n",
        "\n",
        "# Step 3: Split the data into train and test sets (80% train, 20% test)\n",
        "# We use stratify to ensure class distribution remains balanced\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Print shape of splits for confirmation\n",
        "print(\"Training Set Shape:\", X_train.shape)\n",
        "print(\"Test Set Shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "XyRLTQzlxFDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used an 80:20 train-test split to ensure the model has enough data to learn patterns (80%) while keeping a separate portion (20%) to evaluate its performance on unseen data. This balance helps avoid overfitting and gives a reliable estimate of model accuracy."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, the dataset is imbalanced because the number of records for each crime TYPE is not evenly distributed. Some crime types occur far more frequently than others, which can bias the model toward predicting the majority class and reduce accuracy for less frequent crimes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Step 1: Load dataset\n",
        "df = pd.read_excel(\"/content/Train.xlsx\")\n",
        "\n",
        "# Step 2: Split features and target\n",
        "X = df.drop('TYPE', axis=1)\n",
        "y = df['TYPE']\n",
        "\n",
        "# Step 3: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Step 4: Apply Random Over Sampling to balance the training data\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 5: Show class distribution after balancing\n",
        "print(\"Class distribution after oversampling:\")\n",
        "print(y_resampled.value_counts())\n"
      ],
      "metadata": {
        "id": "XpkYAhDCy_vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used Random Oversampling to handle the imbalanced dataset. It increases the number of samples in the minority classes by duplicating them, ensuring all classes have equal representation. This helps the model learn from all crime types fairly and prevents bias toward the majority class."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from xgboost import XGBClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Step 1: Reload the dataset\n",
        "train_df = pd.read_excel(\"/content/Train.xlsx\") # Adjust path if needed\n",
        "\n",
        "# Step 2: Check TYPE column (this will confirm it's intact)\n",
        "print(train_df['TYPE'].value_counts())\n",
        "print(train_df['TYPE'].unique())\n",
        "\n",
        "# Step 3: Define frequent types based on actual class names\n",
        "frequent_types = [\n",
        "    'Break and Enter Commercial',\n",
        "    'Theft from Vehicle',\n",
        "    'Offence Against a Person'\n",
        "]\n",
        "\n",
        "# Step 4: Create binary classification target\n",
        "train_df['TYPE_BINARY'] = train_df['TYPE'].apply(lambda x: 1 if x in frequent_types else 0)\n",
        "\n",
        "# Step 5: Check if TYPE_BINARY is valid now\n",
        "print(train_df['TYPE_BINARY'].value_counts())\n",
        "\n",
        "# Step 6: Prepare features and labels\n",
        "X = train_df.drop(columns=['TYPE', 'TYPE_BINARY', 'Date', 'HUNDRED_BLOCK'])\n",
        "y = train_df['TYPE_BINARY']\n",
        "\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "X = X.select_dtypes(exclude=['datetime64[ns]']).fillna(X.median(numeric_only=True))\n",
        "\n",
        "# Step 7: Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 8: Balance with SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 9: Train model\n",
        "model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "model.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Step 10: Predict and Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "j_RN2BnOAC_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemented an XGBoost Classifier, a powerful gradient boosting algorithm well-suited for structured/tabular data. The model was trained on a binary classification target (TYPE_BINARY), distinguishing between frequent crime types (e.g., Theft from Vehicle, Break and Enter Commercial, Offence Against a Person) and less frequent ones. To address class imbalance, SMOTE (Synthetic Minority Over-sampling Technique) was applied to the training data.\n",
        "The model achieved an overall accuracy of 71.6% on the test set. The precision and recall scores for both classes were balanced (~0.71–0.72), indic"
      ],
      "metadata": {
        "id": "n_-lEWWWDIs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Generate classification report as a dictionary\n",
        "report_dict = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Step 2: Convert report to a DataFrame for display\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "print(\"Classification Report:\")\n",
        "print(report_df[['precision', 'recall', 'f1-score']].round(2))\n",
        "\n",
        "# Step 3: Generate confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 4: Display confusion matrix as heatmap\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Less Frequent (0)', 'Frequent (1)'], yticklabels=['Less Frequent (0)', 'Frequent (1)'])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0KiAuomwC8gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Define the model\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.05, 0.1],\n",
        "    'subsample': [0.8, 1],\n",
        "    'colsample_bytree': [0.8, 1]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit the algorithm\n",
        "grid_search.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Predict on the model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Best Parameters Found:\", grid_search.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "mB5t3ERfD7-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-GridSearchCV for hyperparameter optimization. GridSearchCV performs an exhaustive search over a specified parameter grid and evaluates each combination using cross-validation. This ensures that the model is tuned for the best set of parameters by thoroughly testing all possibilities, which is particularly useful when accuracy and generalization are critical."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-Yes, there was a slight improvement in performance after tuning."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Balance training data using SMOTE\n",
        "sm = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
        "\n",
        "# Step 3: Initialize and train MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=300, random_state=42)\n",
        "mlp.fit(X_train_res, y_train_res)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = mlp.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "rfPD61nbIpBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "Implemented an MLPClassifier (Multilayer Perceptron), a type of feedforward neural network from scikit-learn. It was trained on SMOTE-balanced data to classify crime types into two categories: frequent and less frequent crimes.\n",
        "The model achieved an overall accuracy of 61.2%, which is lower than XGBoost (71.8%). While it showed perfect recall for Class 0 (less frequent crimes), it severely underperformed on Class 1 (frequent crimes) — indicating it is biased toward the majority class."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Classification Report as Dict\n",
        "mlp_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Step 2: Convert to DataFrame\n",
        "mlp_report_df = pd.DataFrame(mlp_report).transpose()\n",
        "mlp_report_df = mlp_report_df[['precision', 'recall', 'f1-score']].round(2)\n",
        "\n",
        "# Step 3: Plot the score chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "mlp_report_df.iloc[:2].plot(kind='bar')\n",
        "plt.title(\"MLPClassifier Performance Metrics (Class 0 vs Class 1)\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.xticks(rotation=0)\n",
        "plt.ylim(0, 1.1)\n",
        "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
        "plt.legend(loc='lower right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nqfUmIEQNEJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-The evaluation metrics used in this ML model—accuracy, precision, recall, and F1-score—provide key insights into its real-world effectiveness and business value. Accuracy indicates the overall correctness of the model, helping law enforcement rely on it for daily decision-making. Precision, especially for frequent crimes, ensures resources are not wasted on false alarms, while recall highlights the model’s ability to capture true high-risk incidents, which is vital for crime prevention. The F1-score balances both and reflects how well the model performs overall. Together, these metrics demonstrate that the model can significantly improve resource allocation, crime prediction, and operational planning, enabling proactive strategies that enhance public safety and reduce crime-related costs."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here- the XGBoost model as the final prediction model because it gave the highest accuracy (71.8%), with balanced precision and recall for both classes. It performed better than other models like MLPClassifier and Logistic Regression, making it the most reliable for predicting frequent and less frequent crime types accurately."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here-I used the XGBoost Classifier as my final model. It’s a powerful gradient boosting algorithm that combines multiple decision trees to make accurate predictions. To understand which features were most influential, I used the model’s built-in feature importance tool. The most impactful features were YEAR, HOUR, NEIGHBOURHOOD, and MONTH, which showed that both time and location play a key role in predicting whether a crime falls under the frequent or less frequent category."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here-Based on the analysis and model evaluation, I conclude that XGBoost is the most effective model for predicting frequent crime types using FBI crime data. With strong performance metrics and reliable feature importance, it supports the business objective of enhancing crime forecasting. The final decision is to adopt this model for real-time prediction and strategic planning, enabling better allocation of law enforcement resources and proactive crime prevention efforts."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}